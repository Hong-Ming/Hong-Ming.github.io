<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>Chordal Graph and Semidefinite Programs</title>
    <meta name="description" lang="en" content="This post is to keep my terminal customization and configuration files so that I can quickly rebuild my terminal in other machine." />
    <link rel="preconnect" href="https://fonts.gstatic.com" />
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;500;700&display=swap" rel="stylesheet" />
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Mono&display=swap" rel="stylesheet" />
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link href="https://fonts.googleapis.com/css2?family=STIX+Two+Text:ital,wght@0,400;0,500;0,700;1,400&display=swap" rel="stylesheet">
    <link href="../../css/fontawesome.css" rel="stylesheet" />
    <link href="../../css/brands.css" rel="stylesheet" />
    <link href="../../css/solid.css" rel="stylesheet" />
    <link href="../../css/regular.css" rel="stylesheet" />
    <link rel="icon" type="image/png" sizes="152x152" href="../../favicon/favicon152.png" />
    <link rel="apple-touch-icon" sizes="152x152" href="../../favicon/favicon152.png" />
    <link rel="stylesheet" href="../../css/main.css" />
    <link rel="stylesheet" href="../../css/posts.css" />
    <link rel="stylesheet" href="../../css/topnav.css" />
    <link rel="stylesheet" href="../../css/footer.css" />
    <link rel="stylesheet" href="../../css/latex.css" />
    <script src="../../js/myscript.js"></script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
    <script>
      window.MathJax = {
        tex: {
          inlineMath: [
            ["$", "$"],
            ["\\(", "\\)"],
          ],
          tags: "ams",
        },
      };
    </script>
    <style>
      :target {
        padding-top: 0px;
        margin-top: 0px;
      }
      .topnav {
        position: relative;
      }
      .clink {
        font-weight: inherit;
        color: inherit;
      }
      p {
        margin-bottom: 5px;
      }
      body {
        text-align: justify;
        font-size: 20px;
      }
      .MathJax {
        font-size: 1em !important;
      }
    </style>
  </head>

  <body>
    <header class="topnav">
      <a id="firstnav" href="https://hong-ming.github.io/">Hong-Ming Chiu</a>
      <div class="dropdown">
        <button class="dropbtn" onclick="topnavFunction()"><i class="fas fa-bars"></i></button>
        <div class="dropdown-content" id="myDropdown">
          <a href="/">Home</a>
          <a href="../../index.html#mybio">Biography</a>
          <a href="../../index.html#myexp">Experiences</a>
          <a href="../../index.html#mypublications">Publications</a>
          <a href="../../index.html#myprojects">Projects</a>
          <a href="/posts/" id="lastnav">Posts</a>
        </div>
      </div>
    </header>

    <main class="page-content">
      <div class="post-wrapper">
        <div class="lg-heading section-head">Chordal Graph and Semidefinite Programs</div>
        <div class="link-path">
          <i class="fas fa-home"></i> <a href="/">Home</a>/<a href="/posts/">Posts</a>/<span style="text-decoration: underline">Chordal Graph and Semidefinite Programs</span>
        </div>

        <div class="post-content">
          <!-- Macros -->
          <!-- prettier-ignore -->
          <span>
            $
            \def\a{{\bf a}}
            \def\b{{\bf b}}
            \def\c{{\bf c}}
            \def\d{{\bf d}}
            \def\q{{\bf q}}
            \def\r{{\bf r}}
            \def\w{{\bf w}}
            \def\x{{\bf x}}
            \def\y{{\bf y}}
            \def\A{{\bf A}}
            \def\B{{\bf B}}
            \def\C{{\bf C}}
            \def\D{{\bf D}}
            \def\H{{\bf H}}
            \def\I{{\bf I}}
            \def\L{{\bf L}}
            \def\N{{\bf N}}
            \def\S{{\bf S}}
            \def\X{{\bf X}}
            \def\Y{{\bf Y}}
            \def\Z{{\bf Z}}
            \def\0{{\bf 0}}
            \def\1{{\bf 1}}
            \def\real{\mathbb R}
            \def\adj{\text{adj}}
            \def\svec{\text{svec}}
            \def\diag{\text{diag}}
            $
          </span>
          <div id="introduction">
            <div class="section" text="The Fundamental Difficulties of Solving SDP"></div>
            <div class="subsection" text="Denseness of SDP"></div>
            <p>
              Given a standard form semidefinite program (SDP) and its dual
              <img id="sdp" src="equ0.png" class="img-center" style="max-width: 100%; margin-bottom: 5px" />
              where $\A_i,\ \C,\ \X,\ \S\in\mathrm{S^n}$, $\b,\ \y\in\real^n$ and $\C\bullet\X=\mathrm{Tr}(\C\X)$. The fundamental difficulty of solving SDP is that the variable $\X$ is generally
              dense, even in the case when $\C$ and $\A_i$'s are sparse. When $\X$ is a dense matrix, the number of variables we need to solve is $n(n+1)/2$. It might seem straightforward to exploit
              the sparsity pattern in the dual problem, where the slack variable $\S$ has the (aggregated) sparsity pattern of $\C,\ \A_1,\ldots ,\A_m$. Nonetheless, compute the gradient of
              log-barrier function $\phi(\S)=\log\det{\S}$ requires the inverse of $\S$, which is generally a dense matrix.
            </p>
            <div class="definition" text="Sparsity Pattern">
              A sparsity pattern $\mathrm{E}=\{(i,j)\}$ is a set of position, and a matrix $\A$ is said to have the sparsity pattern of $\mathrm{V}$ if for all $(i,j)\notin\mathrm{E}$ we have
              $A_{i,j}=0$. It is assume that all the diagonal entires are in $\mathrm{E}$.
            </div>
            <div class="subsection" text="Per-iteration Cost of Interior Point Method is High"></div>
            <p>
              Although SDP can be efficiently solved by interior point method (IMP) in just a few iteration, however, at each iteration of IMP, we have to solve a normal equation \[\H\Delta\y=\r,\] to
              evaluate the Newton step $\Delta\y$. In general, $\H\in\mathrm{S^m}$ is fully dense; therefore $O(m^3)$ time is required to form $\H$ and solve $\Delta\y$, this result in $O(n^6)$ time
              complexity per-iteration because $n\leq m^2$ and requires $O(n^{6.5}\log(1/\epsilon))$ time to obtain the $\epsilon$-accuracy solution using IMP.
            </p>
            <p>
              When the SDP is chordal sparse. That is, the (aggregated) sparsity pattern of $\C,\A_1,\ldots,\A_m$ is chordal sparse. We can achieve lower computational complexity to form $\H$ and
              solve $\H\Delta\y=\r$ at each iteration of interior point method. In this blog post, we elaborate one method to exploit chordal structure in SDP proposed by Richard Y. Zhang in 2020
              <a class="cite" href="https://link.springer.com/article/10.1007/s10107-020-01516-y" target="_blank">1</a>.
            </p>
            <p>
              <span style="font-weight: 700">Notation: </span> Uppercase (lowercase) bold face letters indicate matrices (column vectors). $A_{ij}$ denotes the $(i,j)^{th}$ element of $\A$. $\A
              \succeq 0$ designates $\A$ as a symmetric positive semidefinite matrix. ${\rm Tr}(\cdot)$ denotes the trace of the matrix. $\mathrm{G(V,E)}$ denotes a graph with vertex set $\mathrm V$
              and edge set $\mathrm E\subseteq \mathrm{V\times V}$. $\adj(v)$ denotes a set of adjacent vertices of vertex $v$. $\mathrm{S}^{n}$ is the set of symmetric matrices of order $n$.
              $\mathrm{S_+^n}$ and $\mathrm{S_{++}^n}$ are the sets of positive semidefinite, positive definite matrices, respectively. $\mathrm{S_E^n}$ is the subspace of $\mathrm{S^n}$ of matrices
              with sparsity pattern $\mathrm E$. $\mathrm{S_{E,+}^{n}}$ and $\mathrm{S_{E,++}^{n}}$ are the sets of positive semidefinite and positive definite matrices in $\mathrm{S_E^n}$.
            </p>
          </div>
          <div id="chordal-graph">
            <div class="section" text="Chordal Graph"></div>
            <p>
              In many applications, we can break the underlying problems into solving a series of sparse normal equations \[\A\x=\b,\] where $\A$ is sparse and $\A\succ 0$. A standard approach to
              solve $\x$ is to perform Cholesky factorization on $\A=\L\L^\mathrm{T}$, then solve $\L\d=\b$ following by $\L^\mathrm{T}\x=\d$. Since $\L$ is a lower triangular matrix, solving
              $\L\d=\b$ and $\L^\mathrm{T}\x=\d$ only requires back substitution, the computational complexity to solve $\x$ is cubic time if $\L$ is dense, but achieve linear time if $\L$ is sparse.
              In this section, we will show that if $\A$ has chordal structure, then its sparsity structure remains the same after Cholesky factorization.
            </p>
            <div class="definition" text="Chordal Graph">An undirected graph is chordal if every induce cycle has length 3.</div>
            <div class="subsection" text="Vertex Elimination"></div>
            <p>To understand chordal structure, one must learn the concept of vertex elimination. Given an undirected graph $\mathrm{G(V,E)}$, the vertex elimination is defined as follow.</p>
            <ol style="margin-bottom: 5px">
              <li>Pick any vertex $v\in\mathrm V$ and remove it from the graph.</li>
              <li>Connect all the neighbors of $v$ together.</li>
              <li>Repeat step 1 and 2 until $\mathrm V=\{\emptyset\}$.</li>
            </ol>
            <p>The number of edges introduced during the process of vertex elimination depends on the order of elimination.</p>
            <div class="example" id="exp1">
              Consider a star graph
              <img src="fig0.png" class="img-center" style="max-width: 100%; margin: 5px auto" />
              <p><span style="font-weight: 700">Case 1: </span> elimination order $=(v_1\to v_2\to v_3\to v_4\to v_5)$</p>
              <img src="fig1.png" class="img-center" style="max-width: 100%; margin-bottom: 5px" />
              <p><span style="font-weight: 700">Case 2: </span> elimination order $=(v_2\to v_3\to v_4\to v_5\to v_1)$</p>
              <img src="fig2.png" class="img-center" style="max-width: 100%; margin-bottom: 5px" />
              <p>
                In case 1, we add $6$ edges during the process of elimination. However, in case 2, we don't have to add any edges. We call the elimination order that does not introduce new edges
                during the vertex elimination the perfect elimination ordering.
              </p>
            </div>
            <div class="definition" , text="Perfect Elimination Ordering">
              An elimination order is call the perfect elimination ordering if there is no edge being added during the process of vertex elimination.
            </div>
            <div class="subsection" text="Cholesky Factorization"></div>
            <p>
              he idea of vertex elimination is tightly related to Cholesky factorization. In fact, if the graphical structure of $\A$ has the perfect elimination ordering, the Cholesky factor of $\A$
              will have the same structure if rows and columns of $\A$ are arranged in perfect elimination order. This idea is illustrated as follow.
            </p>
            <div class="example">
              Using the same graph and elimination order in Example <a class="ref" href="#exp1"> 1</a>, and let the number in each vertex denotes the order of elimination.
              <p><span style="font-weight: 700">Case 1: </span> rows and columns of $\A$ are not in perfect elimination ordering</p>
              <img src="fig3.png" class="img-center" style="max-width: 100%; margin-bottom: 5px" />
              <p><span style="font-weight: 700">Case 2: </span> rows and columns of $\A$ are in perfect elimination ordering</p>
              <img src="fig4.png" class="img-center" style="max-width: 100%; margin-bottom: 5px" />
            </div>
            <p>
              The element in matrix that were zeros in $\A$ but become non-zero in $\L$ is called fill-in (the elements marked in red in case 1). In case 1, the Cholesky factor has 6 fill-in and
              become fully dense. However, if we permute the $\A$ into the perfect elimination order as shown in case 2, the Cholesky factor has no fill-in. Here, we give a conceptual explanation for
              this result, for more detail and rigorous proof, see <a class="ref" href="#appendix">Appendix</a>. Partition $\A$ and compute its Schur complement, we have
            </p>
            <img src="equ1.png" class="img-center" style="max-width: 100%; margin-bottom: 5px" />
            <p>
              This expression give us the first column of Cholesky factor $\sqrt{d_1}\begin{bmatrix}1\\\frac{1}{d_1}\b_1\end{bmatrix}$. Obviously, the fill-in are cause by the term
              $\b_1\b_1^\mathrm{T}$ in $\D_1$. Let $\a_1=\begin{bmatrix}d_1\\\b_1\end{bmatrix}$ be the first column of $\A$, because of the term $\b_1\b_1^\mathrm{T}$ in $\D_1$, if $(1,i)$-th and
              $(1,j)$-th elements in $\a_1$ is not zero, $i,j>1$, then the $(i,j)$-th element of $\D_1$ become nonzero. Graphically, it is equivalent to saying that if $(1,i)$, $(1,j)$ are connected,
              then $(i,j)$ will become connected after eliminating vertex $1$, which is exactly the process of vertex elimination. The Cholesky factor can be obtained from performing the similar
              partition on $\D_1$ then compute its Schur complement to obtain $\D_2$, and then repeat this process until we get $\D_n$ (see <a class="ref" href="#appendix">Appendix</a> for details).
              If rows and columns of $\A$ are in perfect elimination order, then each $\D_1,\ldots,\D_n$ will have no fill-in, it follows that the Cholesky factor of $\A$ will have no fill-in as well.
            </p>
            <div class="lemma" text="no fill-in">
              Let $\boldsymbol{A}\succ 0$ be a symmetric matrix. If $\boldsymbol{A}$ is in perfect elimination ordering, then its Cholesky factor $\boldsymbol{L}$ has no fill-in.
            </div>
            <p>In the following, we use the matrix $\A$ in the case 1 above to illustrate this idea. Partition $\A$ and compute its Schur complement, we have</p>
            <img src="equ2.png" class="img-center" style="max-width: 100%; margin-bottom: 5px" />
            <p>The graphical structure of $\C_1-\frac{1}{d_1}\b_1\b_1^\mathrm{T}$ is the graphical structure of $\A$ after eliminating vertex $1$.</p>
            <img src="fig5.png" class="img-center" style="max-width: 100%; margin-bottom: 5px" />
            <p>
              Notice that not every graph has perfect elimination ordering, and the perfect elimination ordering (if exists) is not unique in general. Fulkerson et at.
              <a class="cite" href="https://msp.org/pjm/1965/15-3/pjm-v15-n3-p11-s.pdf">2</a> showed that graphs that have the perfect elimination ordering are exactly the chordal graphs.
            </p>
            <div class="theorem" text="Fulkerson et at. 1965">An undirected graph is chordal if and only if it has the perfect elimination ordering.</div>
            <div class="subsection" text="Clique Tree"></div>
            <p>
              Any chordal graph $\mathrm{G(V,E)}$ can be turn into a tree that has the clique (maximal complete subgraph) of $\mathrm G$ being its vertex. This idea is illustrated in the following
              example borrowed from <a class="cite" href="https://www.seas.ucla.edu/~vandenbe/publications/chordalsdp.pdf" target="_blank">3</a>.
            </p>
            <div class="example" id="exp3">
              The chordal graph in p.29 of <a class="cite" href="https://www.seas.ucla.edu/~vandenbe/publications/chordalsdp.pdf" target="_blank">3</a> and its clique tree. Let $W$ denotes a set of
              cliques.
              <img src="fig6.png" class="img-center" style="max-width: 100%; margin: 5px auto" />
            </div>
            <div class="subsection" text="Induce Subtree, Running Intersection Property"></div>
            <p>
              In Example 3, observe that for any $v\in\mathrm V$, every vertex in the clique tree contains $v$ induce a subtree. Therefore, if $v$ is contained in two cliques $W_1$ and $W_2$, then $v$
              has to be contained in every cliques that are in the path from $W_1$ to $W_2$ on the clique tree. This is known as induce subtree or running intersection property.
            </p>
            <div class="example">
              The induce subtree of vertex $e$ and $c$.
              <img src="fig7.png" class="img-center" style="max-width: 100%; margin-bottom: 5px" />
            </div>
          </div>
          <div id="chordal-graph-and-sdp">
            <div class="section" text="Chordal Graph and Semidefinite Programs"></div>
            <div class="subsection" text="Clique Tree Conversion for SDP"></div>
            <p>
              Suppose $\C, \A_1,\ldots,\A_m\in \mathrm{S_E^n}$.
              <a class="cite" href="https://epubs.siam.org/doi/abs/10.1137/S105262340240851X?mobileUi=0&" target="_blank">4</a>
              <a class="cite" href="https://link.springer.com/article/10.1007/s10107-002-0351-9" target="_blank">5</a>
              <a class="cite" href="https://link.springer.com/article/10.1007/s10107-002-0351-9" target="_blank">6</a>
              <a class="cite" href="https://arxiv.org/abs/cs/0412009" target="_blank">7</a>
              proposed to reformulated SDP as
            </p>
            <img src="equ3.png" class="img-center" style="max-width: 100%; margin-bottom: 5px" />
            <p>where $\mathrm{P_E(S_+^n)}=\left\{\mathrm{P_E}(\X) \mid \X \succeq 0\right\}$ is the cone of matrices in $\mathrm{S_E^n}$ that have a positive semidefinite completion.</p>
            <div class="definition" text="Projection onto Sparsity Pattern">
              A projection $\Y$ of a matrix $\X$ onto a sparsity pattern is denoted $\Y=\mathrm{P_E(\X)}$, i.e., $Y_{i,j}=X_{i,j}$ if $(i,j)\in\mathrm E$ and otherwise $Y_{i,j}=0$.
            </div>
            <p>
              If we further suppose $\mathrm{G(V,E)}$ is chordal and let $W_1,\ldots,W_l$ denotes the cliques of $\mathrm G$, and $\X[W_j,W_j]$ denotes sub-matrices corresponding to clique $W_j$.
              Grone et al. <a class="cite" href="https://www.seas.ucla.edu/~vandenbe/publications/chordalsdp.pdf" target="_blank">7</a> showed that this problem can be further relaxed into the
              optimization problem that only enforce the positive semidefiniteness constraint on each $\X[W_j,W_j]$
            </p>
            <img src="equ4.png" class="img-center" style="max-width: 100%; margin-bottom: 5px" />
            <div class="theorem" text="Grone et al. 1984">
              $\boldsymbol{X}$ is positive semidefinite completable, or $\boldsymbol{X}\in P_E(S_+^n)$, if and only if $\boldsymbol{X}[W_j,W_j]\succeq 0$ for all $j=1,\ldots,l$.
            </div>
            <p>
              By Theorem 2, the relaxation above is always possible, then it follows that the relaxation is tight because $\C\bullet\X=\C\bullet \mathrm{P_E(\X)}$. For simplicity, let
              $\X_j=\X[W_j,W_j]$. For some $\C_j,\A_{ij}\in\real^{|W_j|\times |W_j|}$, we can further rewrite this problem into the optimization problem over $\X_j$'s.
            </p>
            <img id="ctc" src="equ5.png" class="img-center" style="max-width: 100%; margin-bottom: 5px" />
            <p>
              where $\mathcal N_{ij}(X_i)$ denotes the part of $\X_i$ overlaps with $\X_j$. The reformulation above is called the clique tree conversion (CTC) for SDP. Example 5 gives a concrete
              example for this reformulation.
            </p>
            <div class="example">
              Let the graph $\mathrm{G(V,E)}$ shown in the left-hand side be the graphical structure of $\C,\A_1,\ldots,\A_m$. Notice that $\mathrm G$ is chordal and has two cliques $W_1$ and $W_2$.
              The corresponding $\X_1=\X[W_1,W_1]$, $\X_2=\X[W_2,W_2]$ and $\mathcal N_{1,2}(\X_1)=\mathcal N_{2,1}(\X_2)$ are shown in the right-hand side.
              <img src="fig8.png" class="img-center" style="max-width: 100%; margin: 5px auto" />
            </div>
            <p>
              Notice that the $\X$ obtained from (CTC) does not guarantee to be positive semidefinite; therefore, when $\X$ is not positive semidefinite, we need to project $\X$ back to the positive
              semidefinite cone. This process is called positive semidefinite completion and is given as follow
            </p>
            <img id="psd-completion" src="equ6.png" class="img-center" style="max-width: 100%; margin-bottom: 5px" />
            <p>Figure <a class="ref" href="#fig1">1</a>, illustrate the process of solving SDP through the clique tree conversion (CTC) following by the positive semidefinite completion (PSD Completion)</p>
            <img src="fig9.png" class="img-center" style="max-width: 100%; margin-bottom: 5px" />
            <div class="caption" text="Figure 1" id="fig1">
              The graphical illustration of the solving SDP <a class="eqref" href="#sdp">1</a> through clique tree conversion <a class="eqref" href="#ctc">2</a> and positive semidefinite
              completion <a class="eqref" href="#psd-completion">3</a>. The feasible set of <a class="eqref" href="#sdp">1</a> and <a class="eqref" href="#ctc">2</a> are shown in blue and orange
              respectively. $\widehat \X$ denotes an optimizer of <a class="eqref" href="#ctc">2</a>, and $\widehat \Z$ denotes the solution obtained after
              <a class="eqref" href="#psd-completion">3</a>, which is an optimizer of SDP. Obviously, if $\widehat\X$ were in blue set, then we can skip (PSD Completion).
            </div>
            <p>The clique tree conversion can be vectorizes into a linear conic program in standard form</p>
            <img id="vctc" src="equ7.png" class="img-center" style="max-width: 100%; margin-bottom: 5px" />
            <p>
              where $\mathcal K=\mathrm{S^{|W_1|}_+}\times,\ldots,\times, \mathrm{S_+}^{|\mathrm W_l|}$. Let $f(n)=n(n+1)/2$ be a function outputs the number of elements in the lower triangular part
              of $n\times n$ matrix. $\svec(\A)\in\real^{f(n)}$ be the weighted vectorization of the lower triangular part of $n\times n$ matrix $\A$ such that
              $\svec(\A)^\mathrm{T}\svec(\B)=\A\bullet\B$. Then $\c,\x\in\real^{n'}$, $\A\in\real^{m\times n'}$, $n'=\sum_{j=1}^lf(|W_j|)$ is given by
            </p>
            <img src="equ8.png" class="img-center" style="max-width: 100%; margin-bottom: 5px" />
            <p>
              In addition $\N\in\real^{m'\times n'}$ is a block matrix, where $m'=\sum_{i=1}^lf(|W_i\cap W_{p(i)}|)$ and $p(i)$ is the parent of vertex $i$ in the clique tree. $\N$ and its blocks
              $\N_{i,j}\in\real^{f(|W_i\cap W_{p(i)}|)\times f(|W_j|)}$ are defined as
            </p>
            <img src="equ9.png" class="img-center" style="max-width: 100%; margin-bottom: 5px" />
            <div class="subsection" text="Partial Separability"></div>
            <p>The constraint $\sum_{j=1}^l\A_{ij}\bullet\X_j=b_i$ is said to be partially separable if and only if there exists some $\A_i'$ and $i'\in\{1,\ldots,l\}$ such that</p>
            <img src="equ10.png" class="img-center" style="max-width: 100%; margin-bottom: 5px" />
            <div class="example" text="MAX k-CUT">
              Frieze et at. <a class="cite" href="https://link.springer.com/article/10.1007/BF02523688" target="_blank">8</a> proposed a randomized algorithm to approximate MAX k-CUP base on solving
              <img src="equ11.png" class="img-center" style="max-width: 100%; margin-bottom: 5px" />
              Observe that each constraint affect a single matrix element, therefore this problem is partially separable.
            </div>
            <p>When SDP is partially separable, the blocks of matrix $\A$ in <a class="eqref" href="#vctc">4</a> becomes</p>
            <img src="equ12.png" class="img-center" style="max-width: 100%; margin-bottom: 5px" />
            <div class="subsection" text="Dualize Clique Tree Conversion"></div>
            <p>
              Zhang et al. proposed to exploit sparsity structure of $\H$ using the dualizing technique of Lofberg
              <a class="cite" href="https://www.tandfonline.com/doi/abs/10.1080/10556780802553325?journalCode=goms20" target="_blank">9</a>. The dual of <a class="eqref" href="#vctc">4</a> is given by
            </p>
            <img id="normal" src="equ13.png" class="img-center" style="max-width: 100%; margin-bottom: 5px" />
            <p>
              where $\mathcal K^*=\mathcal K$ is the dual cone of $\mathcal K$. The dualization technique of Lofberg
              <a class="cite" href="https://www.tandfonline.com/doi/abs/10.1080/10556780802553325?journalCode=goms20" target="_blank">9</a> swap the role played by primal and dual problem
            </p>
            <img id="lofberg" src="equ14.png" class="img-center" style="max-width: 100%; margin-bottom: 5px" />
            <p>
              where $f=m+m'$ is the number of equality constraint. Observe that the dual problem of <a class="eqref" href="#lofberg">6</a> is exactly the same as the primal problem in
              <a class="eqref" href="#normal">5</a>. Applying the IPM to solve <a class="eqref" href="#lofberg">6</a>. The normal equation we need to solve at each iteration takes the form
            </p>
            <img src="equ15.png" class="img-center" style="max-width: 100%; margin-bottom: 5px" />
            <p>
              where $\D_s$ is a block diagonal matrix such that $\D_s=\diag(\D_{s1},\ldots,\D_{sl})$, $\D_{si}\in\real^{f(|W_i|)\times f(|W_i|)}$ and $\D_{s,i}\succeq 0$. $\D_f$ is low-rank
              perturbation such that $\D_f=\sigma\I+\w\w^\mathrm{T}$. Let
            </p>
            <img src="equ16.png" class="img-center" style="max-width: 100%; margin-bottom: 5px" />
            <p>and apply Sherman-Morrison-Woodbury, we have</p>
            <img id="normal-eq" src="equ17.png" class="img-center" style="max-width: 100%; margin-bottom: 5px" />
            <p>
              The block sparsity pattern of $\N^\mathrm{T}\N$ coincides with the tree graph $\mathrm{G(W,T)}$, and $\A^\mathrm{T}\A$ is a block diagonal matrix under the partially separable
              assumption. As a result, the block sparsity pattern of $\H$ is chordal. Example 7 give a concrete example.
            </p>
            <div class="example">
              Using the chordal graph $\mathrm{G(V,E)}$ and its tree graph $\mathrm{G(W,T)}$ in Example <a class="ref" href="#exp3">3</a> $(n=9,l=6)$. Suppose $m=5$ and $i'=\{2,4,1,2,5\}$, then the
              $\A$ and $\A^\mathrm{T}\A$ has the following structure
              <img src="equ18.png" class="img-center" style="max-width: 100%; margin-bottom: 5px" />
              where $\a_i'=\svec(\A'_i)$, and $\tilde\A_i\in\real^{f(|W_i|)\times f(|W_i|)}$ are some dense matrix. Suppose we index the tree graph $\mathrm{G(W,T)}$ in post-order, then $\N$ and
              $\N^\mathrm{T}\N$ has the following structure
              <img src="equ19.png" class="img-center" style="max-width: 100%; margin-bottom: 5px" />
              where $\tilde\N_{i,j}\in\real^{f(|W_i|)\times f(|W_j|)}$ are some dense matrix. Therefore, $\D_s+\A^\mathrm{T}\A+\N^\mathrm{T}\N$ has the following block structure
              <img src="equ20.png" class="img-center" style="max-width: 100%; margin-bottom: 5px" />
              coincides with the graphical structure of tree graph $\mathrm{G(W,T)}$.
            </div>
            <div class="theorem" text="no block fill" id="thm3">
              Let $\boldsymbol{H}\succ 0$ be a symmetric block matrix. If the block sparsity pattern of $\boldsymbol{H}$ is chordal, then its Cholseky factor has no block fill.
            </div>
            <div class="proof">
              Adapt the proof in <a class="ref" href="#appendix">Appendix</a>. Partition $\H$ and compute its Schur complement, we have
              <img src="equ21.png" class="img-center" style="max-width: 100%; margin-bottom: 5px" />
              The block fill-in is caused by $\B_1\D_{11}^{-1}\B_1^\mathrm{T}$ in $\D_1$. It then follows that when the block sparsity pattern of $\H$ is chordal, then the sparsity pattern of $\C_1$
              will be the same as the sparsity pattern of $\B_1\D_{11}^{-1}\B_1^\mathrm{T}$. The Cholesky factor can be obtained by applying the similar partition on $\D_1$ and computes its Schur
              complement, and then repeat this process until we obtain the block diagonal matrix $\D_n$.
            </div>
            <div class="qed">$\square$</div>
            <p>
              Observe that evaluating $\H^{-1}\q$ is the same as solving $\q'$ such that $\H\q'=\q$. $\q'$ can be solved by first factorizing $\H=\L\L^\mathrm{T}$, and then solve $\L\b=\q$ follows by
              $\L^\mathrm{T}\q'=\b$. By Theorem <a class="ref" href="#thm3">3</a>, $\L$ has no block fill; therefore, $\L$ can be factorized in $O(\beta^3n)$ and $\q'$ can be solved in $O(\beta^2n)$,
              where $\beta=\max_jf(|W_j|)$. This result allows us to evaluate $\Delta\y$ in <a class="eqref" href="#normal-eq">7</a> in $O(\omega^6n)$ time, where $\omega=\max_j|W_j|$ and
              $\beta\leq\omega^2$. As a result, the computational complexity to obtain the $\epsilon$-accuracy solution using IPM is $O(w^6n^{1.5}\log(1/\epsilon))$. Notice Zhang's method is better
              than Andersen's method when the number of equality constraints $m$ is large. Specifically, when $m$ is of $O(\omega n)$ then Andersen's method requires
              $O(\omega^3n^{3.5}\log(1/\epsilon))$ time complexity , which is comparable to the cubic time complexity of direct IMP when the number of constraint $m$ is of $O(n)$.
            </p>
          </div>
          <div id="appendix">
            <div class="section" text="Appendix"></div>
            <div class="appsubsection" text="Proof of Lemma 1"></div>
            <div class="proof">
              Suppose $\A\in\mathrm{S_{E}^n}$. Partition $\A$ and computes its Schur complement, we have
              <img src="equ2.png" class="img-center" style="max-width: 100%; margin-bottom: 5px" />
            </div>
          </div>
          <div id="reference">
            <div class="section" text="Reference"></div>
          </div>
        </div>
      </div>
    </main>

    <footer class="page-footer" style="margin-top: 50px">
      <div class="back-to-top-botton">
        <div class="back-to-top" onclick="SmoothScrollToTop()">Scroll to Top</div>
      </div>
      <div class="myfooter">
        <div class="foot-left">
          <a href="mailto: hongmingchiu0217@gmail.com" class="ficon-button femail"> <i class="fas fa-envelope icon-email"></i><span></span><span id="foot-icon-text">Gmail</span></a>

          <a href="https://www.facebook.com/hmchiu2/" class="ficon-button ffacebook"> <i class="fab fa-facebook-f icon-facebook"></i><span></span><span id="foot-icon-text">Facebook</span></a>

          <a href="https://www.linkedin.com/in/hmchiu/" class="ficon-button flinkedin"> <i class="fab fa-linkedin-in icon-linkedin"></i><span></span><span id="foot-icon-text">LinkedIn</span></a>

          <a href="javascript:void(0)" class="ficon-button ftwitter"> <i class="fab fa-twitter icon-twitter"></i><span></span><span id="foot-icon-text">Twitter</span></a>

          <a href="https://www.instagram.com/_hongming/" class="ficon-button finstagram"> <i class="fab fa-instagram icon-instagram"></i><span></span><span id="foot-icon-text">Instagram</span></a>

          <a href="/sitemap/" class="ficon-button fsitemap"> <i class="fas fa-sitemap icon-sitemap"></i><span></span><span id="foot-icon-text">Sitemap</span></a>
        </div>
        <div class="foot-right">
          <p>
            This is an academic website for Hong-Ming Chiu to share his experiences, publications and projects. Some designs of this website are borrowed and modified from
            <a href="http://hexianghu.com/">Hexiang (Frank) Hu</a>'s.
          </p>
        </div>
      </div>
    </footer>
  </body>
</html>
